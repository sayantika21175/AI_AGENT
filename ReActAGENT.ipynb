{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOZHiC3nrsD9i71JdGy9EWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "731e25447740435f8394ee1a6448ac80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_cba75faa31af4d2e8ef38a06eae43c80"
          }
        },
        "619ab618c4d64ce08e4c2f7dbbaeff95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b1f792d29a4ae1921a3667f9518e37",
            "placeholder": "​",
            "style": "IPY_MODEL_0120dea019f1491dbbf935e9391b346a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "6f12c2ef02e449a7994a929ca20be0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_30d02aab67a9480eb8d1a71fe59cc348",
            "placeholder": "​",
            "style": "IPY_MODEL_90db2c1c5d864422aac1711e155b149f",
            "value": ""
          }
        },
        "ee57fe291b3a4183b56a31eadf7ceae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_f7ed8c0dda574e8d9c248b77c55a530d",
            "style": "IPY_MODEL_283457a7b7604e3da2cf5f0b0166d3f0",
            "value": true
          }
        },
        "2cfc3950f3e9420eb6b50898922f7b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_87338dba59644cbe8b68313b3d798c75",
            "style": "IPY_MODEL_7bf4781a8b734f6891c8f5e3c5b41857",
            "tooltip": ""
          }
        },
        "8ff4ecb6bef94ebea6af6486860dcb2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c300626d5e04090865d8c26b46762e5",
            "placeholder": "​",
            "style": "IPY_MODEL_a2a7a1066a964e6eb040273caee6b54d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "cba75faa31af4d2e8ef38a06eae43c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "b5b1f792d29a4ae1921a3667f9518e37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0120dea019f1491dbbf935e9391b346a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30d02aab67a9480eb8d1a71fe59cc348": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90db2c1c5d864422aac1711e155b149f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7ed8c0dda574e8d9c248b77c55a530d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "283457a7b7604e3da2cf5f0b0166d3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87338dba59644cbe8b68313b3d798c75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf4781a8b734f6891c8f5e3c5b41857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0c300626d5e04090865d8c26b46762e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2a7a1066a964e6eb040273caee6b54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7edc9b0f2064b93b3b6bb0421480fff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8a33a3a5a12464a860a34b9df44e739",
            "placeholder": "​",
            "style": "IPY_MODEL_d9eb7c02145447ccbe682db8beb36eaf",
            "value": "Connecting..."
          }
        },
        "e8a33a3a5a12464a860a34b9df44e739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9eb7c02145447ccbe682db8beb36eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayantika21175/AI_AGENT/blob/main/ReActAGENT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "dXkNOHA9GPPF"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated , List\n",
        "import operator\n",
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "memory=SqliteSaver.from_conn_string(\":memory:\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "LsmeOJJ32u7q"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %pip install -U langgraph langsmith langchain_anthropic\n",
        "# !pip install langgraph-checkpoint-sqlite\n",
        "# !pip install tavily-python"
      ],
      "metadata": {
        "id": "xgzgc3SGgknZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agentstate(TypedDict):\n",
        "  task:str\n",
        "  plan:str\n",
        "  draft:str\n",
        "  critique:str\n",
        "  content:List[str]\n",
        "  revision_no: int\n",
        "  max_iter: int"
      ],
      "metadata": {
        "id": "b0Aa9SpfGwbQ"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import transformers\n",
        "\n",
        "# pipeline= transformers.pipeline(\"text-generation\", model=\"microsoft/phi-4\",model_kwargs={\"torch_dtype\": \"auto\"},device_map=\"auto\")"
      ],
      "metadata": {
        "id": "TH7WzIO_iO_Y"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.random.manual_seed(0)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\n",
        "#     \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=\"auto\",\n",
        "#     trust_remote_code=True,\n",
        "# )\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "lb_g55332qRM"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pipe=pipeline(\"text-generation\",model=model,tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "IZkTNgV23v1C"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation_args = {\n",
        "#     \"max_new_tokens\": 500,\n",
        "#     \"return_full_text\": False,\n",
        "#     \"temperature\": 0.0,\n",
        "#     \"do_sample\": False,\n",
        "# }"
      ],
      "metadata": {
        "id": "Bcr_Q5XksmkH"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Plan_prompt=\"\"\"you are an expert writer tasked with writting a high level outline of an essay.\\\n",
        "Write such an outline for the user provided topic. give an outline of the essay along with any relevant notes\\\n",
        "or instructions for the sections.\"\"\"\n"
      ],
      "metadata": {
        "id": "vevnlkdfjFOV"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Writer_prompt=\"\"\"you are as essay assistant tasked with writing excellent 5 paragraph essays.\\\n",
        "generate the best essay possible for the user's request and the initial outline.\\\n",
        "if the user provides critique, respond with a revised version of your previous attempts .\\\n",
        "utilise all the information below as needed:\n",
        "--------\n",
        "\n",
        "{content}\"\"\""
      ],
      "metadata": {
        "id": "z_r1Ll89m9e_"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Refelection_prompt=\"\"\"you are a teacher grading an essay submission.\\\n",
        "Generate critique and recommendations for the user's submission.\\\n",
        "Provide detailed recommendations, including requests for length, depth, style, etc\"\"\""
      ],
      "metadata": {
        "id": "8mIBA1O-nrDv"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Research_plan_prompt=\"\"\"you are a researcher chargded with providing information that can \\\n",
        "be used when writing the following essay. Generate a list of search queries that will gather\\\n",
        "any relevant information. only generate 3 queries max\"\"\""
      ],
      "metadata": {
        "id": "7zRSqHgQ5TXH"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "research_critique_prompt=\"\"\"you are a researcher charged with providing information that can\\\n",
        "be used when making any requested revisions (as outlined below).\\\n",
        "Generate a list of search quaries that will gather any relevant information. only generate 3 queries max\"\"\""
      ],
      "metadata": {
        "id": "gigl59xxoAFM"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel,Field\n",
        "\n",
        "class Queries(BaseModel):\n",
        "  queries:List[str]\n",
        "# class Queries(BaseModel):\n",
        "#     setup: str = Field(description=\"The list of the strings\")"
      ],
      "metadata": {
        "id": "R-qQGb2GoZI9"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Joke(BaseModel):\n",
        "#     setup: str = Field(description=\"The setup of the joke\")"
      ],
      "metadata": {
        "id": "n4TBAnhKWbff"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pydantic import BaseModel\n",
        "# class Queries(BaseModel):\n",
        "#     '''An answer to the user question along with justification for the answer.'''\n",
        "#     answer: list[str]"
      ],
      "metadata": {
        "id": "S_Le0EEkZtu6"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "# app_key=userdata.get('hugging_face')\n",
        "# #app_key"
      ],
      "metadata": {
        "id": "1DV73e7N3BFS"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import userdata\n",
        "# google_key=userdata.get('googl_api')\n",
        "# #app_key"
      ],
      "metadata": {
        "id": "EF-JlsODPTZZ"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -qU langchain-google-vertexai"
      ],
      "metadata": {
        "id": "AnoNrHhGPjn3"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD9JhEttPyFJ",
        "outputId": "022c3af6-3240-4740-a3a0-f312b3945c96"
      },
      "execution_count": 235,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain_mistralai"
      ],
      "metadata": {
        "id": "KPQF9e-BdrF0"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "model_mistral = ChatMistralAI(model=\"mistral-large-latest\")"
      ],
      "metadata": {
        "id": "glZBRE0Cd9XV"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_google_vertexai import ChatVertexAI\n",
        "\n",
        "# chat = ChatVertexAI(model=\"gemini-pro\")"
      ],
      "metadata": {
        "id": "EVrIAxtbQX3x"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "# login(app_key)"
      ],
      "metadata": {
        "id": "eTpPUvSh3s5T"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "# llm = HuggingFaceEndpoint(\n",
        "#     repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "#     task=\"text-generation\",\n",
        "#     max_new_tokens=512,\n",
        "#     do_sample=False,\n",
        "#     repetition_penalty=1.03,\n",
        "# )\n",
        "\n",
        "# chat = ChatHuggingFace(llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "JcNnd4I-zWBh"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.output_parsers import PydanticOutputParser\n",
        "# parser=PydanticOutputParser(pydantic_object=Queries)"
      ],
      "metadata": {
        "id": "zwWbl98L46QB"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "tavily=TavilyClient(api_key=\"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\")"
      ],
      "metadata": {
        "id": "IHdEyS7woiO5"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tavily.search(\"what is langchain\",max_results=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5Xt9Eq849ma",
        "outputId": "1fc0618c-4c10-49d6-b9c2-b8c83e6e3853"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'what is langchain',\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'title': 'Introduction | ️ LangChain',\n",
              "   'url': 'https://python.langchain.com/docs/introduction/',\n",
              "   'content': \"How to use tools in a chain How to migrate from legacy LangChain agents to LangGraph How to use chat models to call tools LangChain is a framework for developing applications powered by large language models (LLMs). Development: Build your applications using LangChain's open-source building blocks, components, and third-party integrations. langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. LangServe: Deploy LangChain chains as REST APIs. LangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications. Build stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\",\n",
              "   'score': 0.92176014,\n",
              "   'raw_content': None},\n",
              "  {'title': 'Introduction to LangChain - GeeksforGeeks',\n",
              "   'url': 'https://www.geeksforgeeks.org/introduction-to-langchain/',\n",
              "   'content': 'Data Structures and Algorithms\\nML & Data Science\\nWeb Development\\nLanguages\\nInterview Corner\\nCS Subjects\\nJobs\\nPractice\\nContests\\nIntroduction to Langchain\\nLangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). LangChain follows a general pipeline where a user asks a question to the language model where the vector representation of the question is used to do a similarity search in the vector database and the relevant information is fetched from the vector database and the response is later fed to the language model. LangChain Key Concepts:\\nThe main properties of LangChain Framework are :\\nSetting up the environment\\nInstallation of langchain is very simple and similar as you install other libraries using the pip command.\\n Next, we create a .env file and store our API key in it as follows:\\nPython\\nNow, I am creating a new file named ‘lang.py’ where I will be using the LangChain framework to generate responses. We start by importing long-chain and initializing an LLM as follows:\\nPython\\nWe are initializing it with a high temperature which means that the results will be random and less accurate.',\n",
              "   'score': 0.92074,\n",
              "   'raw_content': None}],\n",
              " 'response_time': 1.8}"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plan_node(state:Agentstate):\n",
        "  messages=[\n",
        "      (\"system\",Plan_prompt),\n",
        "      (\"human\",state[\"task\"])\n",
        "  ]\n",
        "  response=model_mistral.invoke(messages)\n",
        "  return {\"plan\":response.content}"
      ],
      "metadata": {
        "id": "uzaqXQsso31J"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_research_plan(state:Agentstate):\n",
        "  messages=model_mistral.with_structured_output(Queries).invoke([\n",
        "      (\"system\",Research_plan_prompt),\n",
        "      (\"human\",state[\"task\"])]\n",
        "  )\n",
        "  #structured_output=parser.parse(messages.content)\n",
        "  content=state.get(\"content\", [])\n",
        "  for q in messages.queries:\n",
        "    result=tavily.search(q,max_results=2)\n",
        "    for i in result['results']:\n",
        "      content.append(i['content'])\n",
        "  return {\"content\":content}"
      ],
      "metadata": {
        "id": "_2ZE8ekEqUIm"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages=model_mistral.with_structured_output(Queries).invoke([\n",
        "      (\"system\",\"you are an assistant. give proper answer in three lines\"),\n",
        "      (\"human\",\"what is llm?\")\n",
        "  ])\n",
        "# structured_output=parser.parse(str(messages.content))\n",
        "# structured_output\n",
        "messages\n",
        "#print(parser.parse(\"what is your name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAi8wgcs6YB5",
        "outputId": "347b3586-7d08-4a52-c8bc-259446d29f13"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Queries(queries=['what is llm'])"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generation_node(state: Agentstate):\n",
        "  content=\"\\n\\n\".join(state[\"content\"] or [])\n",
        "  user_message=(\"human\",f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
        "  messages=[\n",
        "      (\"system\",Writer_prompt.format(content=content)),\n",
        "      user_message\n",
        "  ]\n",
        "  response=model_mistral.invoke(messages)\n",
        "  print(f\"current revision number: {state.get('revision_no')}\")\n",
        "  return {\"draft\":response.content,\n",
        "          \"revision_no\": state.get(\"revision_no\",1)+1}\n"
      ],
      "metadata": {
        "id": "Od4tdF_T8tKk"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reflection_node(state:Agentstate):\n",
        "  messages=[\n",
        "      (\"system\",Refelection_prompt),\n",
        "      (\"human\",state[\"draft\"])\n",
        "  ]\n",
        "  response=model_mistral.invoke(messages)\n",
        "  return {\"critique\":response.content}"
      ],
      "metadata": {
        "id": "rRq0X8VTLkSF"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_critique_node(state:Agentstate):\n",
        "  messages=model_mistral.with_structured_output(Queries).invoke([\n",
        "      (\"system\",research_critique_prompt),\n",
        "      (\"human\",state[\"critique\"])\n",
        "  ])\n",
        "  content=state.get(\"content\", [])\n",
        "  for q in messages.queries:\n",
        "    result=tavily.search(q,max_results=2)\n",
        "    for i in result['results']:\n",
        "      content.append(i['content'])\n",
        "  return {\"content\":content}\n"
      ],
      "metadata": {
        "id": "HQBb2wYuMbW-"
      },
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state:Agentstate):\n",
        "  if state[\"revision_no\"]>state[\"max_iter\"]:\n",
        "    return END\n",
        "  else:\n",
        "    return \"reflect\""
      ],
      "metadata": {
        "id": "eD7SjuctN73q"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder=StateGraph(Agentstate)"
      ],
      "metadata": {
        "id": "Rr3tR0IyPJOz"
      },
      "execution_count": 344,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()"
      ],
      "metadata": {
        "id": "tWu4bLw2gikO"
      },
      "execution_count": 345,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder.add_node(\"planner\",plan_node)\n",
        "builder.add_node(\"research_plan\",get_research_plan)\n",
        "builder.add_node(\"generate\",generation_node)\n",
        "builder.add_node(\"reflection\",reflection_node)\n",
        "builder.add_node(\"research_critique\",get_critique_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a869Gu1BPTW4",
        "outputId": "f672933c-309e-4631-9088-853319dfd2d7"
      },
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7dcd855d7950>"
            ]
          },
          "metadata": {},
          "execution_count": 346
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "builder.set_entry_point(\"planner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOLlhQ4OQAp3",
        "outputId": "d3ac06b8-20de-4eda-fcb7-b8af0f056ec1"
      },
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7dcd855d7950>"
            ]
          },
          "metadata": {},
          "execution_count": 347
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "builder.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    should_continue,\n",
        "    {END:END,\"reflect\":\"reflection\"}\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDcvrqIYQEBE",
        "outputId": "e3f6dbcf-11df-4eec-8361-2af103aa8976"
      },
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7dcd855d7950>"
            ]
          },
          "metadata": {},
          "execution_count": 348
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "builder.add_edge(\"planner\",\"research_plan\")\n",
        "builder.add_edge(\"research_plan\",\"generate\")\n",
        "\n",
        "builder.add_edge(\"reflection\",\"research_critique\")\n",
        "builder.add_edge(\"research_critique\",\"generate\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y2GdQhHQmIx",
        "outputId": "64599e8d-6771-4ddb-a891-4be0b9f8b1a8"
      },
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7dcd855d7950>"
            ]
          },
          "metadata": {},
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
        "graph=builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "a1yQbSp1Q8JP"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
        "\n",
        "display(\n",
        "    Image(\n",
        "        graph.get_graph().draw_mermaid_png(\n",
        "            draw_method=MermaidDrawMethod.API,\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 587
        },
        "id": "cBXE_BV2RBy0",
        "outputId": "c2ef3559-959e-4f77-cc17-47d31495cc46"
      },
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gEOHz7gkqz3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "qEbNyJiLm04s"
      },
      "execution_count": 352,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# messages = [\n",
        "#     {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
        "#     {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
        "# ]\n",
        ""
      ],
      "metadata": {
        "id": "maxYGZlPs0QU"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread={\"configurable\":{\"thread_id\":\"1\"}}\n",
        "response=\"\"\n",
        "\n",
        "for s in graph.stream(\n",
        "    {\n",
        "        'task':\"What is the difference between Langchain and Langsmith?\",\n",
        "        \"max_iter\":2,\n",
        "        \"revision_no\":1\n",
        "    },thread):\n",
        "  print(s)\n",
        "\n",
        "  # if isinstance(s,SystemMessage):\n",
        "  #   response+=s.content\n",
        "  # elif isinstance(s,str):\n",
        "  #   response+=s\n",
        "  # else:\n",
        "  #   response+=str(s)\n",
        "  # print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPBEdMwccpzn",
        "outputId": "e7f3f078-c421-4304-dc73-e94132451959"
      },
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'planner': {'plan': '**Essay Outline: The Difference between LangChain and LangSmith**\\n\\n**I. Introduction**\\n  - A. Briefly introduce the concept of large language models (LLMs) and their increasing importance in AI.\\n  - B. Introduce LangChain and LangSmith as two prominent tools in the LLM ecosystem.\\n  - C. Thesis Statement: While LangChain and LangSmith both serve to enhance the capabilities of LLMs, they differ in their core functionalities, use cases, and approaches to approach LLM implementation.\\n\\n**II. Understanding LangChain**\\n  - A. **Definition and Purpose**\\n     - 1. Define LangChain and its primary goals.\\n     - 2. Explain its role in facilitating the interaction between LLMs and other tools or data sources.\\n  - B. **Key Features**\\n     - 1. Modularity: Discuss how LangChain allows users to combine different components to create custom LLM applications.\\n     - 2. Chains: Explain the concept of \"chains\" in LangChain, which allow for sequential processing of language tasks.\\n     - 3. Agents: Describe how LangChain enables the creation of agents that use LLMs to interact with external tools and APIs.\\n  - C. **Use Cases**\\n     - 1. Provide examples of typical use cases for LangChain, such as creating chatbots, automating data analysis, or generating text based on complex prompts.\\n\\n**III. Understanding LangSmith**\\n  - A. **Definition and Purpose**\\n     - 1. Define LangSmith and its primary goals.\\n     - 2. Explain its role in evaluating and improving the performance of LLMs.\\n  - B. **Key Features**\\n     - 1. Evaluation: Discuss LangSmith\\'s capabilities for evaluating the performance of LLMs on various tasks and benchmarks.\\n     - 2. Fine-tuning: Explain how LangSmith facilitates the fine-tuning of LLMs to improve their performance on specific tasks.\\n     - 3. Data Management: Describe LangSmith\\'s tools for managing and preparing data for LLM training and evaluation.\\n  - C. **Use Cases**\\n     - 1. Provide examples of typical use cases for LangSmith, such as benchmarking LLM performance, fine-tuning models for specific domains, or preparing data for LLM training.\\n\\n**IV. Comparing LangChain and LangSmith**\\n  - A. **Core Functionalities**\\n     - 1. Compare the core functionalities of LangChain and LangSmith, highlighting how LangChain focuses on facilitating LLM interactions while LangSmith emphasizes LLM evaluation and improvement.\\n  - B. **Use Cases**\\n     - 1. Discuss the different use cases where LangChain or LangSmith might be more appropriate.\\n  - C. **Compatibility and Integration**\\n     - 1. Explore how LangChain and LangSmith can be used together to create more powerful LLM applications.\\n\\n**V. Conclusion**\\n  - A. **Recap** the differences and complementary nature of LangChain and LangSmith.\\n  - B. **Reiterate** the importance of understanding these tools for effective LLM implementation.\\n  - C. **Future Outlook**: Briefly discuss the potential future developments for LangChain and LangSmith as the field of LLMs continues to evolve.\\n\\n**Notes for each section:**\\n\\n- Ensure that each section flows logically into the next, maintaining a clear and coherent structure.\\n- Use specific examples and case studies to illustrate the points made in each section.\\n- Cite relevant sources and academic papers to support the arguments and provide credibility.\\n- Avoid jargon and technical terms that may not be familiar to the intended audience; if technical terms are used, ensure they are clearly explained.\\n- In the conclusion, consider providing a concise table or diagram that summarizes the key differences between LangChain and LangSmith for quick reference.\\n\\nThis outline provides a high-level structure for the essay, ensuring that all key aspects of LangChain and LangSmith are covered and compared effectively.'}}\n",
            "{'research_plan': {'content': ['If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready', 'LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.']}}\n",
            "current revision number: 1\n",
            "{'generate': {'draft': '### The Difference between LangChain and LangSmith\\n\\n#### Introduction\\n\\nIn the rapidly evolving field of artificial intelligence (AI), large language models (LLMs) have emerged as a cornerstone, revolutionizing various industries by enabling advanced natural language processing capabilities. Among the tools that enhance the capabilities of LLMs, LangChain and LangSmith stand out. While both serve to augment LLM functionalities, they differ significantly in their core functionalities, use cases, and approaches to LLM implementation. This essay aims to delve into the distinctions between LangChain and LangSmith, highlighting their unique roles and how they complement each other in the LLM ecosystem.\\n\\n#### Understanding LangChain\\n\\nLangChain is designed to facilitate the interaction between LLMs and other tools or data sources, making it an invaluable asset for early-stage prototyping and small-scale applications. Its primary goal is to manage and scale model workflows efficiently. One of LangChain\\'s key features is its modularity, which allows users to combine different components to create custom LLM applications tailored to specific needs. This modular approach ensures flexibility and adaptability, making LangChain suitable for a wide range of use cases.\\n\\nAnother critical feature of LangChain is the concept of \"chains.\" Chains enable sequential processing of language tasks, allowing for complex workflows that can handle multiple steps in a coherent manner. Additionally, LangChain enables the creation of agents that use LLMs to interact with external tools and APIs. These agents can automate tasks such as data analysis, content generation, and more, making LangChain ideal for applications like chatbots, automated customer service, and text generation based on complex prompts. For instance, a chatbot built with LangChain can seamlessly integrate with various data sources and APIs to provide accurate and contextually relevant responses.\\n\\n#### Understanding LangSmith\\n\\nLangSmith, on the other hand, is focused on evaluating and improving the performance of LLMs, making it better suited for large-scale, production-ready applications. Its primary goal is to provide deep visibility and control over complex AI systems in production. LangSmith\\'s advanced debugging and orchestration features are indispensable for managing intricate AI pipelines with multiple models that require frequent debugging and monitoring. This makes LangSmith essential for ensuring that AI models work reliably in production environments.\\n\\nOne of LangSmith\\'s standout features is its evaluation capabilities. It allows for comprehensive performance assessments of LLMs on various tasks and benchmarks, ensuring that models meet the required standards before deployment. Additionally, LangSmith facilitates the fine-tuning of LLMs to improve their performance on specific tasks, making it an excellent tool for domain-specific applications. LangSmith also offers robust data management tools, enabling users to prepare and manage data effectively for LLM training and evaluation. For example, a financial institution using LangSmith can fine-tune models to better understand and predict market trends, ensuring accurate and reliable insights.\\n\\n#### Comparing LangChain and LangSmith\\n\\nWhile LangChain and LangSmith serve different purposes, they are complementary tools that can be used together to create more powerful LLM applications. LangChain excels at managing and scaling model workflows, making it ideal for early-stage prototyping and small-scale applications. In contrast, LangSmith is designed for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly.\\n\\nIn practical application, LangChain can be used to build pipelines, while LangSmith can be employed to monitor and debug them. For instance, a developer might use LangChain to create a complex AI pipeline for a recommendation system and then use LangSmith to ensure that the pipeline performs as expected in a production environment. This complementary approach allows for seamless integration and effective management of LLM-based workflows.\\n\\n#### Conclusion\\n\\nIn conclusion, LangChain and LangSmith are two essential tools in the LLM ecosystem, each with its unique strengths and use cases. LangChain is ideal for early-stage prototyping and small-scale applications, offering modularity, chains, and agents to facilitate LLM interactions. Conversely, LangSmith is better suited for large-scale, production-ready applications, providing advanced debugging, evaluation, and data management capabilities. Understanding the differences and complementary nature of these tools is crucial for effective LLM implementation. As the field of LLMs continues to evolve, the synergy between LangChain and LangSmith will likely play a pivotal role in advancing AI capabilities and applications. By leveraging both tools effectively, developers can build and fine-tune robust LLM-based systems that meet the demands of modern AI applications.', 'revision_no': 2}}\n",
            "{'reflection': {'critique': '### Critique and Recommendations\\n\\n#### Strengths\\n\\n1. **Clear Introduction**: The essay begins with a clear introduction that sets the context and purpose of the comparison between LangChain and LangSmith.\\n2. **Structured Organization**: The essay is well-organized, with separate sections for understanding each tool and a comparative analysis.\\n3. **Detailed Explanations**: Each tool is explained in detail, highlighting their key features and use cases.\\n4. **Practical Examples**: The use of practical examples helps to illustrate the applications of LangChain and LangSmith effectively.\\n\\n#### Areas for Improvement\\n\\n1. **Length and Depth**:\\n   - **Recommendation**: The essay could benefit from more depth in certain areas. For instance, the section on LangSmith could be expanded to include more specific examples of its evaluation and debugging capabilities.\\n   - **Request**: Add at least one more paragraph to each section (LangChain and LangSmith) to provide more detailed insights and examples.\\n\\n2. **Comparative Analysis**:\\n   - **Recommendation**: The comparative analysis section is somewhat brief. It would be helpful to delve deeper into how LangChain and LangSmith can be integrated in practical scenarios.\\n   - **Request**: Expand the comparative analysis section by at least two paragraphs, providing more detailed examples of how the tools can complement each other in different stages of AI development and deployment.\\n\\n3. **Style and Clarity**:\\n   - **Recommendation**: While the essay is generally well-written, some sentences are overly complex and could be simplified for better readability.\\n   - **Request**: Review the essay for complex sentences and break them down into simpler, more concise statements. For example, instead of saying \"LangChain is designed to facilitate the interaction between LLMs and other tools or data sources, making it an invaluable asset for early-stage prototyping and small-scale applications,\" you could say: \"LangChain is designed to facilitate interactions between LLMs and other tools or data sources. This makes it an invaluable asset for early-stage prototyping and small-scale applications.\"\\n\\n4. **Conclusion**:\\n   - **Recommendation**: The conclusion is well-written but could be strengthened by summarizing the key points more succinctly and providing a stronger closing statement.\\n   - **Request**: Rewrite the conclusion to include a brief summary of the key differences and complementary nature of LangChain and LangSmith. End with a forward-looking statement about the future of LLMs and the role of these tools.\\n\\n5. **Citations and References**:\\n   - **Recommendation**: The essay lacks citations and references, which are crucial for academic writing.\\n   - **Request**: Include at least three to five reputable sources to support the claims made in the essay. Ensure that all references are properly cited in a consistent format (e.g., APA, MLA, or Chicago style).\\n\\n6. **Transitions**:\\n   - **Recommendation**: The transitions between sections could be smoother to improve the flow of the essay.\\n   - **Request**: Use transitional phrases to connect ideas between sections. For example, when moving from the section on LangChain to LangSmith, you could use a phrase like \"In contrast to LangChain\\'s focus on prototyping and small-scale applications, LangSmith targets large-scale, production-ready environments.\"\\n\\n#### Specific Feedback\\n\\n1. **Introduction**:\\n   - **Suggestion**: Consider adding a hook to engage the reader from the start. For example, you could begin with a compelling statistic or quote about the impact of LLMs in modern industries.\\n\\n2. **Understanding LangChain**:\\n   - **Suggestion**: Provide a real-world example of how LangChain\\'s modularity and chains have been used in a specific application. This could be a case study or a hypothetical scenario.\\n\\n3. **Understanding LangSmith**:\\n   - **Suggestion**: Include a detailed example of how LangSmith\\'s evaluation and debugging capabilities have been used to improve the performance of an LLM in a production environment.\\n\\n4. **Comparing LangChain and LangSmith**:\\n   - **Suggestion**: Discuss potential challenges or limitations of using LangChain and LangSmith together. This could include issues related to integration, data management, or scalability.\\n\\n5. **Conclusion**:\\n   - **Suggestion**: End with a call to action or a thought-provoking question to encourage further exploration of the topic.\\n\\n#### Overall Recommendations\\n\\n- **Length**: Aim for a total length of around 1,500 to 2,000 words to provide a comprehensive analysis.\\n- **Depth**: Ensure that each section provides detailed insights and examples to support the claims made.\\n- **Style**: Maintain a clear and concise writing style, avoiding overly complex sentences.\\n- **Citations**: Include at least three to five reputable sources to support your arguments.\\n- **Transitions**: Use transitional phrases to improve the flow of the essay.\\n\\nBy addressing these areas, you can enhance the quality and depth of your essay, making it more informative and engaging for the reader.'}}\n",
            "{'research_critique': {'content': ['If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Additionally, if you’re working on cross-platform model deployments — say, running models on-prem and in the cloud simultaneously — Langsmith offers better orchestration and monitoring tools to handle the complexity.', 'LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready', 'LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities', 'Here’s the deal: LangChain is like building the entire car, while LangSmith is your diagnostic tool to ensure that car runs smoothly. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', \"Langchain vs LlamaIndex: A Comparative Analysis. ... LangChain's integrations, such as LangSmith for evaluation and LangServe for deployment, enhance the development lifecycle by providing tools for streamlined deployment processes and optimization. On the other hand, LlamaIndex integrates external knowledge sources and databases as query\", 'This image shows the architecture of the LangChain framework | source: Langchain documentation The LangChain ecosystem comprises the following: LangSmith: This helps you trace and evaluate your language model applications and intelligent agents, helping you move from prototype to production.; LangGraph: is a powerful tool for building stateful, multi-actor applications with LLMs.', 'Utilize LangSmith for tracing and debugging to pinpoint where the output diverges from expectations. Integration Challenges. Problem: Difficulty integrating LangChain with external data sources or APIs. Solution: Ensure that the external sources are correctly configured and accessible. Review the official documentation on data connection', 'When working with LangChain, users might encounter a variety of issues ranging from installation problems to complex integration challenges. This section aims to address some of the most common issues, providing clear solutions and workarounds. ... Debugging LangChain Applications. Using LangSmith for Tracing: LangSmith is a powerful tool for', \"The company has 70+ businesses in fields such as\\nLangChain Partners with CommandBar on their Copilot User Assistant\\nCommandBar is a user assistance platform that helps software companies make their products easy to use by capturing and predicting user intent, and then delivering\\nLangChain partners with Elastic to launch the Elastic AI Assistant\\nElastic, a leading search analytics company, serving over 20k customers worldwide, enables organizations to securely harness search-powered AI so anyone can find the answers they\\nAlly Financial Collaborates with LangChain to Deliver Critical Coding Module to Mask Personal Identifying Information in a Compliant and Safe Manner\\nAlly Financial, the largest digital-only bank in the US and a leading auto lender, has recently collaborated with LangChain to release the first initial coding\\nLLMs accelerate Adyen's support team through smart-ticket routing and support agent copilot\\nChallenge\\nAs global commerce accelerates, Adyen, a publicly-traded financial technology platform, is helping large companies like Meta, Uber, H&M, and Microsoft achieve their\\nLangChain Expands Collaboration with Microsoft\\n LangChain helps developers build context-aware reasoning applications and powers some of the most\\nMorningstar Intelligence Engine puts personalized investment insights at analysts' fingertips\\nChallenge\\nFinancial services is one of the most data-driven industries and financial professionals are always hungry for more data and better tools to drive value\\nRobocorp’s code generation assistant makes building Python automation easy for developers\\nChallenge\\nRobocorp was founded in 2019 out of frustration that the promise of developers being able to automate monotonous work hadn’t been realized. Case Studies\\nRakuten Group builds with LangChain and LangSmith to deliver premium products for its business clients and employees\\nRakuten Group is well known for operating one of the largest online shopping malls in Japan. Today, we’re thrilled to announce a collaboration between LangChain and Microsoft.\\n Right\\n© LangChain Blog 2024\", '|The| article| \"|LL|M| Powered| Autonomous| Agents|\"| by| Lil|ian| W|eng| discusses| the| development| and| capabilities| of| autonomous| agents| powered| by| large| language| models| (|LL|Ms|).| It| outlines| a| system| overview| that| includes| three| main| components|:| planning|,| memory|,| and| tool| use|.| |1|.| **|Planning|**| involves| task| decomposition|,| where| agents| break| down| complex| tasks| into| manageable| sub|go|als|,| and| self|-ref|lection|,| allowing| agents| to| learn| from| past| actions| to| improve| future| performance|.|2|.| **|Memory|**| is| categorized| into| short|-term| and| long|-term| memory|,| with| techniques| like| Maximum| Inner| Product| Search| (|M|IPS|)| used| for| efficient| information| retrieval|.|3|.| **|Tool| Use|**| highlights| the| integration| of| external| APIs| to| enhance| the| agent|\\'s| capabilities|,| illustrated| through| case| studies| like| Chem|Crow| for| scientific| discovery| and| Gener|ative| Agents| for| sim|ulating| human| behavior|.|The| article| also| addresses| challenges| such| as| finite| context| length|,| difficulties| in| long|-term| planning|,| and| the| reliability| of| natural| language| interfaces|.| It| concludes| with| references| to| various| studies| and| projects| that| contribute| to| the| field| of| L|LM|-powered| agents|.||']}}\n",
            "current revision number: 2\n",
            "{'generate': {'draft': '**The Difference between LangChain and LangSmith**\\n\\n**I. Introduction**\\n\\nIn the rapidly evolving field of artificial intelligence, large language models (LLMs) have emerged as a cornerstone, revolutionizing various industries by enabling sophisticated natural language processing capabilities. Two prominent tools in the LLM ecosystem are LangChain and LangSmith, each designed to enhance the capabilities of LLMs but differing in their core functionalities, use cases, and approaches to LLM implementation. This essay aims to explore the distinctions between LangChain and LangSmith, providing a comprehensive understanding of their roles and how they complement each other in the development and deployment of LLM-based applications.\\n\\n**II. Understanding LangChain**\\n\\n**A. Definition and Purpose**\\n\\nLangChain is a framework designed to facilitate the interaction between LLMs and other tools or data sources. Its primary goal is to make it easier for developers to build and manage complex LLM applications by providing a structured approach to combining different components. LangChain acts as a bridge, connecting LLMs with external APIs, databases, and other tools to create cohesive and functional applications.\\n\\n**B. Key Features**\\n\\n1. **Modularity**: LangChain\\'s modular architecture allows users to mix and match different components to create custom LLM applications tailored to specific needs. This modularity ensures flexibility and scalability, making it suitable for a wide range of use cases.\\n2. **Chains**: One of LangChain\\'s core concepts is \"chains,\" which enable sequential processing of language tasks. Chains allow developers to link multiple LLM components together, creating a pipeline that can handle complex workflows efficiently.\\n3. **Agents**: LangChain facilitates the creation of agents that use LLMs to interact with external tools and APIs. These agents can perform tasks such as data retrieval, automation, and decision-making, enhancing the capabilities of LLM applications.\\n\\n**C. Use Cases**\\n\\nLangChain is ideal for early-stage prototyping and small-scale applications. Typical use cases include creating chatbots that can engage in natural conversations, automating data analysis tasks, and generating text based on complex prompts. For example, a developer might use LangChain to build a chatbot that can answer customer queries by integrating an LLM with a customer support database.\\n\\n**III. Understanding LangSmith**\\n\\n**A. Definition and Purpose**\\n\\nLangSmith, on the other hand, is focused on evaluating and improving the performance of LLMs. Its primary goal is to provide deep visibility and control over large, complex AI systems in production. LangSmith is designed for those times when you need to debug, monitor, and orchestrate multiple models and workflows, ensuring that LLM applications run smoothly and efficiently.\\n\\n**B. Key Features**\\n\\n1. **Evaluation**: LangSmith offers advanced capabilities for evaluating the performance of LLMs on various tasks and benchmarks. This evaluation helps identify areas where the model may need improvement, ensuring optimal performance.\\n2. **Fine-tuning**: LangSmith facilitates the fine-tuning of LLMs to improve their performance on specific tasks. By adjusting the model\\'s parameters, developers can enhance its accuracy and relevance for particular use cases.\\n3. **Data Management**: LangSmith provides tools for managing and preparing data for LLM training and evaluation. Effective data management is crucial for ensuring that the model has access to high-quality data, which is essential for its performance.\\n\\n**C. Use Cases**\\n\\nLangSmith is better suited for large-scale, production-ready applications. Typical use cases include benchmarking LLM performance to identify areas for improvement, fine-tuning models for specific domains such as finance or healthcare, and preparing data for LLM training. For instance, a financial institution might use LangSmith to fine-tune an LLM for detecting fraudulent transactions, ensuring that the model is highly accurate and reliable.\\n\\n**IV. Comparing LangChain and LangSmith**\\n\\n**A. Core Functionalities**\\n\\nThe core functionalities of LangChain and LangSmith highlight their distinct roles in the LLM ecosystem. LangChain focuses on facilitating interactions between LLMs and other tools, making it easier to build and manage complex applications. In contrast, LangSmith emphasizes the evaluation and improvement of LLMs, providing the tools necessary to ensure that models perform optimally in production environments.\\n\\n**B. Use Cases**\\n\\nThe use cases for LangChain and LangSmith differ based on the stage of LLM development and the specific requirements of the application. LangChain is ideal for early-stage prototyping and small-scale applications, where flexibility and modularity are essential. LangSmith, however, is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.\\n\\n**C. Compatibility and Integration**\\n\\nLangChain and LangSmith can be used together to create more powerful LLM applications. By leveraging LangChain\\'s capabilities to build and manage complex workflows and LangSmith\\'s tools for evaluation and fine-tuning, developers can ensure that their LLM applications are both functional and optimized for performance. This complementary approach allows for a more comprehensive and effective LLM implementation.\\n\\n**V. Conclusion**\\n\\nIn conclusion, LangChain and LangSmith serve distinct but complementary roles in the LLM ecosystem. LangChain excels at managing and scaling model workflows, making it ideal for early-stage prototyping and small-scale applications. LangSmith, with its advanced debugging and orchestration features, is essential for ensuring that large, complex AI systems run smoothly in production. Understanding the differences and complementary nature of these tools is crucial for effective LLM implementation. As the field of LLMs continues to evolve, future developments for LangChain and LangSmith will likely focus on enhancing their capabilities and integration, making them even more powerful tools for developers and researchers alike.\\n\\n**Summary Table**\\n\\n| Feature          | LangChain                                      | LangSmith                                     |\\n|------------------|------------------------------------------------|-----------------------------------------------|\\n| **Core Functionality** | Facilitates LLM interactions with other tools | Evaluates and improves LLM performance       |\\n| **Use Cases**    | Early-stage prototyping, small-scale applications | Large-scale, production-ready applications    |\\n| **Key Features** | Modularity, Chains, Agents                    | Evaluation, Fine-tuning, Data Management     |\\n| **Compatibility** | Can be integrated with LangSmith for enhanced functionality | Complements LangChain for optimal performance|\\n\\nThis table summarizes the key differences between LangChain and LangSmith, providing a quick reference for understanding their roles and capabilities.', 'revision_no': 3}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yW_8nz0ovxBq"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a helpful assistant. provide the answer precisely.If you do not know say you do not know.\"),\n",
        "    (\"human\", \"What is the difference between Langchain and Langsmith?.\"),\n",
        "]\n",
        "\n",
        "res=chat.invoke(messages)\n",
        "#structured_op=parser.parse(res.content)\n",
        "#structured_op"
      ],
      "metadata": {
        "id": "8HaIySRdxWaB"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res['AIMessage']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "q8P5AIn3ySW7",
        "outputId": "b70bc46d-40fd-4c3a-b754-5ca592609e2f"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'AIMessage' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-625d6c92b853>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AIMessage'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'AIMessage' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "L28Fc6qizGjI",
        "outputId": "446d211b-25cc-4694-cba7-b490000168a7"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Langchain and Langsmith are not widely recognizable entities as of my knowledge cutoff in 2023, which might be due to them being relatively new concepts, private projects, or specific tools within certain communities, or even having similar names that cause some confusion.\\n\\nIf you are referring to software frameworks or development tools in AI and programming fields, some of the likely options you might be considering could be \"Langchain\" and \"Lambda School\".\\n\\nLangchain might be a term you would come across in discussions about AI and data processing, referring to a chain of language processing functions. However, without specific information, there\\'s no standard definition or application. You might be talking about a particular proprietary technology at a company, such as Google\\'s Langchain, which is a new tech product that has been rumored to be focused on high-performance AI applications using Google\\'s technologies.\\n\\nLambda School, on the other hand, is a very well-known online platform. It offers career-focused coding bootcamps to help people learn software development, starting with no coding experience.\\n\\nIf you meant a different set of tools or frameworks with these names and have more context, please provide additional details.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain-huggingface"
      ],
      "metadata": {
        "id": "MvCT2onwv2Bx"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login() # You will be prompted for your HF key, which will then be saved locally"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "731e25447740435f8394ee1a6448ac80",
            "619ab618c4d64ce08e4c2f7dbbaeff95",
            "6f12c2ef02e449a7994a929ca20be0d7",
            "ee57fe291b3a4183b56a31eadf7ceae0",
            "2cfc3950f3e9420eb6b50898922f7b77",
            "8ff4ecb6bef94ebea6af6486860dcb2f",
            "cba75faa31af4d2e8ef38a06eae43c80",
            "b5b1f792d29a4ae1921a3667f9518e37",
            "0120dea019f1491dbbf935e9391b346a",
            "30d02aab67a9480eb8d1a71fe59cc348",
            "90db2c1c5d864422aac1711e155b149f",
            "f7ed8c0dda574e8d9c248b77c55a530d",
            "283457a7b7604e3da2cf5f0b0166d3f0",
            "87338dba59644cbe8b68313b3d798c75",
            "7bf4781a8b734f6891c8f5e3c5b41857",
            "0c300626d5e04090865d8c26b46762e5",
            "a2a7a1066a964e6eb040273caee6b54d",
            "e7edc9b0f2064b93b3b6bb0421480fff",
            "e8a33a3a5a12464a860a34b9df44e739",
            "d9eb7c02145447ccbe682db8beb36eaf"
          ]
        },
        "id": "XwU4dqeWv980",
        "outputId": "210e2025-6612-4678-c3dc-7138303e28b0"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "731e25447740435f8394ee1a6448ac80"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thread={\"configurable\":{\"thread_id\":\"1\"}}"
      ],
      "metadata": {
        "id": "wnEMj9kClWco"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = \"\"\n",
        "\n",
        "for s in graph.stream(\n",
        "    {\n",
        "        'task': \"What is the difference between Langchain and Langsmith?\",\n",
        "        \"max_iter\": 3,\n",
        "        \"revision_number\": 1\n",
        "    },\n",
        "    thread\n",
        "):\n",
        "    try:\n",
        "        # Attempt to extract content if it has a content attribute\n",
        "        response += getattr(s, \"content\", str(s))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing: {s}, Exception: {e}\")\n",
        "\n",
        "    print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "6AGv4A41lTfM",
        "outputId": "73f0fbd6-063c-4480-e63e-a90a68e3b13c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can only concatenate str (not \"SystemMessage\") to str",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-34f193e70871>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m for s in graph.stream(\n\u001b[0m\u001b[1;32m      4\u001b[0m     {\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"What is the difference between Langchain and Langsmith?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1668\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   1671\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    172\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m                 )\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-c3c407148f4f>\u001b[0m in \u001b[0;36mplan_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   ]\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"plan\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     def preprocess(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1280\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m                 )\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             )\n\u001b[1;32m    309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt_text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"SystemMessage\") to str"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WGVjVogpd7Rk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}